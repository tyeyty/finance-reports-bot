import os
import requests
from bs4 import BeautifulSoup

# KCIF ì£¼ê°„/ì›”ê°„ ë³´ê³ ì„œ í˜ì´ì§€ URL
BASE_URL = "https://www.kcif.or.kr"
TARGET_URL = "https://www.kcif.or.kr/annual/monthlyList"  # ì›”ê°„ë³´ê³ ì„œ
# TARGET_URL = "https://www.kcif.or.kr/annual/weeklyList"  # ì£¼ê°„ë³´ê³ ì„œ (ë°”ê¾¸ë©´ ì£¼ê°„ìœ¼ë¡œ ì „í™˜ ê°€ëŠ¥)

# ì €ì¥ í´ë” ì„¤ì •
SAVE_DIR = "./downloads"
os.makedirs(SAVE_DIR, exist_ok=True)

def get_pdf_links():
    """KCIF ë³´ê³ ì„œ í˜ì´ì§€ì—ì„œ PDF ë§í¬ë“¤ì„ ê°€ì ¸ì˜¤ê¸°"""
    response = requests.get(TARGET_URL)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "html.parser")

    pdf_links = []
    for a_tag in soup.find_all("a", href=True):
        href = a_tag["href"]
        if href.endswith(".pdf"):
            full_url = href if href.startswith("http") else f"{BASE_URL}{href}"
            pdf_links.append(full_url)
    return pdf_links

def download_pdfs(links):
    """PDF íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ"""
    for link in links:
        file_name = link.split("/")[-1]
        file_path = os.path.join(SAVE_DIR, file_name)
        if os.path.exists(file_path):
            print(f"âœ… ì´ë¯¸ ì¡´ì¬: {file_name}")
            continue
        print(f"â¬‡ï¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {file_name}")
        pdf_data = requests.get(link).content
        with open(file_path, "wb") as f:
            f.write(pdf_data)
        print(f"ğŸ“ ì €ì¥ ì™„ë£Œ: {file_path}")

def main():
    print("ğŸ” KCIF ë³´ê³ ì„œ ë§í¬ íƒìƒ‰ ì¤‘...")
    pdf_links = get_pdf_links()
    if not pdf_links:
        print("âŒ PDF ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return
    print(f"ğŸ“„ ì´ {len(pdf_links)}ê°œ íŒŒì¼ ë°œê²¬")
    download_pdfs(pdf_links)
    print("ğŸ‰ ëª¨ë“  ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
